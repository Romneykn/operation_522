{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import linalg as spla\n",
    "from scipy.io import loadmat\n",
    "import graphlearning as gl\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Indian Pines Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN search with Annoy approximate nearest neighbor package...\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "def get_dataset():\n",
    "    img = loadmat('Indian_pines_corrected.mat')['indian_pines_corrected']\n",
    "    gt = loadmat('Indian_pines_gt.mat')['indian_pines_gt']\n",
    "    \n",
    "    # Save dataset\n",
    "    X = img.reshape(-1, 200)\n",
    "    gl.save_dataset(X, 'indian_pines', overwrite=True)\n",
    "\n",
    "    # Save labels\n",
    "    L = gt.reshape(-1)\n",
    "    gl.save_labels(L, 'indian_pines', overwrite=True)\n",
    "\n",
    "    # To add a dataset to the simulation environment, we also need\n",
    "    # to save a label permutation, which is a number of random train/test splits\n",
    "    # and store some precomputed knn-data\n",
    "\n",
    "    # Create label permutation with 100 trials at 1,2,3,4,5 labels per class\n",
    "    # You can add any identifying string as name='...' if you need to create additional\n",
    "    # label permutations for a dataset.\n",
    "#     gl.create_label_permutations(L, 100, [1,2,3,4,5], dataset='indian_pines', name=None, overwrite=True)\n",
    "\n",
    "    # Run knn search and save info on 30 nearest neighbors\n",
    "    # Choose as many as you are likely to use in practice, the code will automatically subset if needed.\n",
    "    # This uses a kd-tree. For higher dimensional data use the annoy package, as below\n",
    "    I, J, D = gl.knnsearch_annoy(X, 30, dataset='indian_pines')\n",
    "    # I, J, D = gl.knnsearch(X, 30, dataset='indian_pines')\n",
    "    return I, J, D,img\n",
    "    \n",
    "I, J, D, img = get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Overview of Active Learning from Kevin's presentation\n",
    "\n",
    "<img src=\"high_level_overview.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(iters = 100,added_label = 25):\n",
    "#   initialize\n",
    "    labels = gl.load_labels('indian_pines')\n",
    "    train_ind = gl.randomize_labels(labels, 10)\n",
    "    tau = .1\n",
    "    idxs = range(len(list(labels)))\n",
    "    pred_labels = []\n",
    "    # Get Weight Matrix and Adjusted Graph Laplacian\n",
    "    W = gl.weight_matrix(I, J, D, 10)\n",
    "    L = gl.graph_laplacian(W, norm='none') + tau**2*gl.sparse.identity(W.shape[0])\n",
    "#     while acc < .01:\n",
    "        # Run SSL\n",
    "    accuracy = []\n",
    "    percentage = []\n",
    "    i = 0\n",
    "    while i < iters:\n",
    "        train_labels = labels[train_ind]\n",
    "        pred_labels = gl.graph_ssl(W, train_ind, train_labels, algorithm='laplace')\n",
    "        # Calculate Accuracy\n",
    "        acc = np.sum(pred_labels == labels) / len(labels)\n",
    "        accuracy.append(acc)\n",
    "#         print('Accuracy:', acc)\n",
    "        idxs  = list(set(idxs) - set(train_ind))\n",
    "        new_ind = random.choices(idxs,k = added_label)\n",
    "        train_ind = list(train_ind) + new_ind \n",
    "        percentage.append(len(train_ind)/len(labels))\n",
    "        i+=1\n",
    "    return accuracy,percentage,pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc,perc,pred = loop()\n",
    "\n",
    "plt.plot(acc,label = 'Accuracy')\n",
    "plt.plot(perc,label = 'Percentage of Data Labeled')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = loadmat('Indian_pines_gt.mat')['indian_pines_gt']\n",
    "plt.subplots(12)\n",
    "plt.subplot(121)\n",
    "plt.title('Ground Truth')\n",
    "plt.imshow(img)\n",
    "plt.subplot(122)\n",
    "plt.title(122)\n",
    "plt.title('Predicted')\n",
    "plt.imshow(pred.reshape(145,145))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "The idea behind active learning is that you initialy train a model with a very few labeled elements and then identify the datapoints that would have the most effect on your model if you did know the labels of that data. Then with an expert or 'oracle' you label those points and retrain the model. You repeat this process and are able to acheive a high level of accuracy while having to actively label a small percentage of the data.\n",
    "\n",
    "For our attempt of this method we took in an image of fields in Indian Pines, Indiana. The image has 200 channels thus each pixel has 200 features. Using those features we group the pixels and by their grouping, experts are able to tell what sort of crop is in a field.\n",
    "\n",
    "For our attempt we used Jeff Calder's `graphlearning` package to quickly group the data using a Laplacian Graph based Semi-Supervised Model. For the update step we did something similar to what Kevin did during his presentation. Kevin Miller demonstrated that usually the most 'helpful' peices of data were grouped together so if you were to only add the 25 most helpful data points at each iteration you would just be adding a clumped group of data. This would not be very helpful because after the most helpful peice was added, the points grouped around it would no longer be as helpful. It is not necessarily computationaly sensible to calculate the most helpful data point after the addition of each labeled peice of data. We had significant trouble attempting to calculate the 'most helpful points' using the equations presented in Kevin's presentation, so we decided at each stage to add a random assortment of points from the data and then use those points to improve our model. \n",
    "\n",
    "There is initially a very impressive return on investment but as with most investments there is the law of diminishing returns and eventually the improvement added by every iteration is quite unimpressive. Clearly if we could use Active learning, the return would be much accelerated with much more impressive results.\n",
    "\n",
    "The bulk of our troubles resolved around the parameter `A` used in his presentation. We weren't quite sure how to determine that parameter for the multiclass version of the data. Calculating the Hessian also presented a challenge. We did however thoroughly digest his method and feel if we had received a response to our inquiries to him, we would have quickly had much more success."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
